{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9txEESvQvS5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (2.32.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.5 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PfLs_UJQvTE1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u7IStj6uvTJN"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "             \"Three years later, the coffin was still full of Jello.\",\n",
    "             \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "             \"The person box was packed with jelly many dozens of months later.\",\n",
    "             \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "             \"It took him a month to finish the meal.\",\n",
    "             \"Finishing the meal took him 3 weeks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_6_zQDuzvTRp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a34232bdbb347b2aad4076998121812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b8e91c69f44a588f08bcf2fcea1271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861f7df70f23406998c2f5c152fcfb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd701fe3cc84987b5a201b8332798ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb26827026141b59ce7cafb316d4a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0b07429b4b4f64b2d23bbf40aa2ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8578a147a3b494f849c0b22a09dec9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o4QMUKcWvTT7"
   },
   "outputs": [],
   "source": [
    "def compute_tokens(sentences, tokenizer):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    # encoding all sentences for bert input\n",
    "    for sentence in sentences:\n",
    "        sentence_encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(sentence_encoding['input_ids'][0])\n",
    "        attention_mask.append(sentence_encoding['attention_mask'][0])\n",
    "    \n",
    "    # stacking all the input_ids and attention_mask along 1 dim\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "    # final shape of input_ids & attention_mask = torch.Size([6, 128]), initially they were list.\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W8j79hrAvTWO"
   },
   "outputs": [],
   "source": [
    "def compute_sentence_vector(tokens, model):\n",
    "    \n",
    "    last_hidden_state, pooled_output = model(**tokens, return_dict=False)\n",
    "    '''\n",
    "    Now let's apply mean pooling on last_hidden_state vector of shape torch.Size([6,128,768])\n",
    "    to convert it into meaningful sentence embedding.\n",
    "\n",
    "    For this we need to create a sentence vector by multiplying the attention_mask\n",
    "    with last_hidden_state so that we ignore non-real tokens i.e. ignore padding tokens. \n",
    "    \n",
    "    The final sentence vector will have 768 embeddings for those\n",
    "    words where there was 1 else 0 = padding tokens.\n",
    "    In order to multiply, we need to expand attention_mask dim by 1 so that both becomes [6,128,768].\n",
    "    '''    \n",
    "    attention_mask = tokens['attention_mask'].unsqueeze(-1).expand(last_hidden_state.shape).float()\n",
    "    masked_embeddings = last_hidden_state * attention_mask \n",
    "\n",
    "    # applying mean pooling\n",
    "    '''\n",
    "    This pooling operation will take the mean of all token embeddings and compress them into a \n",
    "    single 768 vector space — creating a ‘sentence vector’.\n",
    "\n",
    "    At the same time, we can’t just take the mean activation as is. We need to consider \n",
    "    null padding tokens (which we should not include).\n",
    "    '''\n",
    "    summed = torch.sum(masked_embeddings, dim=1) # shape = [6,768]\n",
    "    counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9) # shape = [6,768]\n",
    "    mean_pooled_embedding = summed / counts # shape = [6, 768] i.e. our final sentence vector.\n",
    "\n",
    "    return mean_pooled_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nBswqqpWvTYm"
   },
   "outputs": [],
   "source": [
    "def compute_similarity(sentences, tokenizer, model):\n",
    "    \n",
    "    sentences_tokens = compute_tokens(sentences, tokenizer)\n",
    "    sentences_embeddings = compute_sentence_vector(sentences_tokens, model)\n",
    "    sentences_embeddings_detached = sentences_embeddings.detach().numpy()\n",
    "    similarity_scores = cosine_similarity([sentences_embeddings_detached[0]], sentences_embeddings_detached[1:])\n",
    "\n",
    "    d = {\n",
    "        'column-1': [sentences[0] for _ in range(len(sentences)-1)],\n",
    "        'column-2': [sent for sent in sentences[1:]],\n",
    "        'scores': similarity_scores[0]\n",
    "    }\n",
    "\n",
    "    output = pd.DataFrame(data=d)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BsOqTYM_vTbq"
   },
   "outputs": [],
   "source": [
    "output = compute_similarity(sentences, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4UbkWdKUvTgQ",
    "outputId": "bca3fbdd-9a7e-42ed-8420-b9d7964a66e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column-1</th>\n",
       "      <th>column-2</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>The fish dreamed of escaping the fishbowl and ...</td>\n",
       "      <td>0.330889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>The person box was packed with jelly many doze...</td>\n",
       "      <td>0.721926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>Standing on one's head at job interviews forms...</td>\n",
       "      <td>0.174755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>It took him a month to finish the meal.</td>\n",
       "      <td>0.447097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Three years later, the coffin was still full o...</td>\n",
       "      <td>Finishing the meal took him 3 weeks.</td>\n",
       "      <td>0.579585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            column-1  \\\n",
       "0  Three years later, the coffin was still full o...   \n",
       "1  Three years later, the coffin was still full o...   \n",
       "2  Three years later, the coffin was still full o...   \n",
       "3  Three years later, the coffin was still full o...   \n",
       "4  Three years later, the coffin was still full o...   \n",
       "\n",
       "                                            column-2    scores  \n",
       "0  The fish dreamed of escaping the fishbowl and ...  0.330889  \n",
       "1  The person box was packed with jelly many doze...  0.721926  \n",
       "2  Standing on one's head at job interviews forms...  0.174755  \n",
       "3            It took him a month to finish the meal.  0.447097  \n",
       "4               Finishing the meal took him 3 weeks.  0.579585  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentence similarity using BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
